<?xml version="1.0" encoding="UTF-8"?>
<!--
 Copyright 2022, UNSW (ABN 57 195 873 179)

 SPDX-License-Identifier: BSD-2-Clause
-->

<!--
@tim: explain what role each of the VM's and UIO have in this example.

QEMU provides the hardware emulation, for which we use the generic virtual 
platform configured with an ARM Cortex-A53 CPU.  This CPU is chosen to mirror 
the ODROID-C2, which is historically a common development platform for seL4.  
Current development is focussed on the ODROID-C4, which uses the successor of 
the A53, the ARM Cortex-A55. 


Refs: 
- https://qemu-project.gitlab.io/qemu/system/arm/virt.html
- https://wiki.odroid.com/odroid-c2/odroid-c2
- https://wiki.odroid.com/odroid-c4/odroid-c4

On ARM architectures, Linux relies on a device tree to provide information 
about the hardware.  This allows the same compiled kernel to support different 
hardware configurations.  Device trees can be specified in human readable, DTS 
source files, which are then compiled into DTB blob files.  The boat loader 
hands this blob to the kernel.

In order to configure our system we rely on certain information contained in
the device tree.  For example, the Linux guest will expect to find certain 
devices at certain addresses, and associate particular interrupt numbers with 
those devices. So it's necessary to provide these mappings in the system 
configuration. 

Refs:
- https://www.devicetree.org/

We can get a dump of the platform devicetree by executing qemu with the 
`-machine dumptdb=<file>` option:

```
qemu-system-aarch64 -machine virt,virtualization=on,dumpdtb=qemu.dtb \
    -cpu cortex-a53 \
    -serial mon:stdio \
    -m size=1G \
    -nographic \
    -netdev user,id=mynet0 \
    -device virtio-net-device,netdev=mynet0,mac=52:55:00:d1:55:01
```

We can then decompile the DTB dump into a human-readable DTS file with
the device tree compiler:

```
dtc -I dtb -O dts qemu.dtb -o qemu.dts
```
-->

<system>
    <!-- 
    We allocate 256MiB of RAM for each VM. This is fairly arbitrary, it's 
    likely more than each guest will need, and typically far less than the 
    available physical memory.

    We specify large 2MiB pages to reduce kernel memory usage and the
    time required to initialise the system, as mapping each page currently
    requires a separate system call.  The guest can still nominate their own
    page size, which will be managed by the hardware.

    We specify the physical address of the driver guest RAM as this will need
    to match the virtual address where it's mapped into the driver guest VM and
    VMM.  This is because the guest is using DMA for real passthrough devices,
    so its view of memory needs to align with the physical memory.  Currently
    the VMM presents its view of memory to the guest, so naturally all 3 need 
    to align.
    -->
    <memory_region name="driver_guest_ram" size="0x10_000_000" page_size="0x200_000" 
        phys_addr="0x40_000_000" />
    <memory_region name="client_guest_ram" size="0x10_000_000" page_size="0x200_000" />

    <!-- 
    We need to create a memory region for the Ethernet device.  Technically
    this example doesn't rely on Ethernet, but it will be used in systems 
    built on this example (e.g. using UIO with an Ethernet driver, or mounting 
    NFS within the guest to access user programs).

    To determine the expected start address and size of the memory region we
    need to inspect the devicetree, which describes the system hardware.
    This example uses the qemu generic virtual platform, which supports 32 
    virtio-mmio transport devices, one of which will be used for Ethernet.
    
    We can get a dump of the platform devicetree by executing qemu with the 
    `-machine dumptdb=<file>` option:

    ```
    qemu-system-aarch64 -machine virt,virtualization=on,dumpdtb=qemu.dtb \
        -cpu cortex-a53 \
        -serial mon:stdio \
        -m size=1G \
        -nographic \
        -netdev user,id=mynet0 \
        -device virtio-net-device,netdev=mynet0,mac=52:55:00:d1:55:01
    ```

    We can then decompile the DTB dump into a human-readable DTS file with:

    ```
    dtc -I dtb -O dts qemu.dtb -o qemu.dts
    ```

    Inspection of the DTS will reveal details of the 32 virtio-mmio regions.

    Ref: https://qemu-project.gitlab.io/qemu/system/arm/virt.html
    
    Currently we rely on trial-and-error to determine how Linux maps each region
    to a device.

    @tim: Can we figure out which region Linux will use for each device?

    The DTS node for the Ethernet device is:

    ```
    virtio_mmio@a003e00 {
        dma-coherent;
        interrupts = <0x00 0x2f 0x01>;
        reg = <0x00 0xa003e00 0x00 0x200>;
        compatible = "virtio,mmio";
    };
    ```

    This tells us the expected physical address of the device (0xa003e00) and 
    the size of the region (0x200).  However the hardware has a minimum page 
    size of 4 KiB (i.e. 0x1000), so we instead create a 4 KiB memory region from 
    0xa003000.  Also worth noting that if no `page_size` is specified for a 
    `memory_region` then it defaults to 4 KiB.  The `size` obviously needs to 
    be a multiple of the `page_size`.

    The DTS also provides information about the interrupt that will be needed
    when mapping the IRQ into the driver guest VM.  In particular, that it's an
    SPI interrupt (0x00), with interrupt number 0x2f, that is edge triggered (0x01).
    Note that as an SPI interrupt, the GIC will naturally add 0x20 (32) before 
    trapping into seL4.  Therefore we need to map the IRQ into the guest at 
    0x4f (79).
    -->
    <memory_region name="eth" size="0x1_000" phys_addr="0xa_003_000" />

    <!--
    We map in the UART serial device for I/O in the client guest, which is 
    useful for testing and debugging.  We can once again look at the devicetree:

    pl011@9000000 {
        clock-names = "uartclk\0apb_pclk";
        clocks = <0x8000 0x8000>;
        interrupts = <0x00 0x01 0x04>;
        reg = <0x00 0x9000000 0x00 0x1000>;
        compatible = "arm,pl011\0arm,primecell";
    };

    This tells us we need a region of size 0x1000 with base address 0x9000000.
    It also tells us it's an SPI interrupt (0x00), with number 0x01 (+32 = 33
    when mapped into the guest), that is level triggered (0x04).
    -->
    <memory_region name="uart" size="0x1_000" phys_addr="0x9_000_000"/>

    <!--
    We specify 3 UIO regions of 8KiB arbitrarily.  These are similarly 
    specified arbitrarily in the DTS:

    ```
    uio {
        compatible = "generic-uio\0uio";
        reg = <0x00 0x30000000 0x00 0x2000 0x00 0x30002000 0x00 0x2000 0x00 0x30004000 0x00 0x2000>;
        interrupts = <0x00 0x0a 0x04>;
    };
    ```

    The generic UIO driver will setup paths for each in the file system at 
    /dev/uio0, /dev/uio1, /dev/uio2.

    We don't specify physical addresses for each region as we don't care where
    the seL4CP tool positions them in physical memory.  We do however specify
    base addresses in the DTS, which will need to correspond to the virtual
    address mappings in the guests.  Also note that each guest could be built
    with a different DTS, with different base addresses, and correspondingly
    different mappings here in the system description.  For simplicity, however,
    we use the same DTS overlay for both.

    @tim: Separate out the UIO (and Ethernet?) nodes and the base DTS?  
    @tim: Write a script to cat the overlays and check ordering is correct?
    -->
    <memory_region name="uio_map0" size="0x2_000" />
    <memory_region name="uio_map1" size="0x2_000" />
    <memory_region name="uio_map2" size="0x2_000" />

    <!-- 
    To boot Linux, we need to map in the GIC virtual CPU interface.  
    
    @tim: Why, more specifically?

    We can similarly find the relevant hardware information in the DTS:

    ```
    intc@8000000 {
        phandle = <0x8002>;
        interrupts = <0x01 0x09 0x04>;
        reg = <0x00 0x8000000 0x00 0x10000 
               0x00 0x8010000 0x00 0x10000 
               0x00 0x8030000 0x00 0x10000 
               0x00 0x8040000 0x00 0x10000>;
        compatible = "arm,cortex-a15-gic";
        ranges;
        #size-cells = <0x02>;
        #address-cells = <0x02>;
        interrupt-controller;
        #interrupt-cells = <0x03>;

        v2m@8020000 {
            phandle = <0x8003>;
            reg = <0x00 0x8020000 0x00 0x1000>;
            msi-controller;
            compatible = "arm,gic-v2m-frame";
        };
    };
    ```

    The 4 memory regions are the distributor registers, the CPU
    interface registers, the virtual interface control registers, and the 
    virtual CPI interface registers.  It's this last region with base address 
    0x8040000 that we need to map, and we specify a single 4 KiB page, as the 
    register map doesn't use the full 64 KiB specified in the DTS.

    Ref: https://www.kernel.org/doc/Documentation/devicetree/bindings/interrupt-controller/arm%2Cgic.txt
    -->
    <memory_region name="gic_vcpu" size="0x1_000" phys_addr="0x8_040_000" />

    <protection_domain name="driver_guest_vmm">
        <program_image path="driver_guest_vmm.elf" />
        <map mr="driver_guest_ram" vaddr="0x40_000_000" perms="rw" setvar_vaddr="guest_ram_vaddr" />
        <map mr="uio_map0" vaddr="0x30_000_000" perms="rw" cached="false" />
        <map mr="uio_map1" vaddr="0x30_002_000" perms="rw" cached="false" />
        <map mr="uio_map2" vaddr="0x30_004_000" perms="rw" cached="false" />
        
        <virtual_machine name="driver_guest" id="0">
            <map mr="driver_guest_ram" vaddr="0x40_000_000" perms="rwx" />
            <map mr="uio_map0" vaddr="0x30_000_000" perms="rw" cached="false" />
            <map mr="uio_map1" vaddr="0x30_002_000" perms="rw" cached="false" />
            <map mr="uio_map2" vaddr="0x30_004_000" perms="rw" cached="false" />
            <map mr="gic_vcpu" vaddr="0x8_010_000" perms="rw" cached="false" />
            <map mr="uart" vaddr="0x9_000_000" perms="rw" cached="false" />

            <map mr="eth" vaddr="0xa_003_000" perms="rw" cached="false" />
        </virtual_machine>

        <!-- Serial IRQ -->
        <irq irq="33" id="1" />
        <!-- Ethernet IRQ -->
        <irq irq="79" id="2" trigger="edge" />
    </protection_domain>

    <!-- 
    For simplicity we map the client guest RAM at the same addresses as the 
    driver guest, but they can be distinct.
    -->
    <protection_domain name="client_guest_vmm">
        <program_image path="client_guest_vmm.elf" />
        <map mr="client_guest_ram" vaddr="0x40_000_000" perms="rw" setvar_vaddr="guest_ram_vaddr" />
        <!-- <map mr="uio_map0" vaddr="0x30_000_000" perms="rw" cached="false" setvar_vaddr="uio_map0" />
        <map mr="uio_map1" vaddr="0x30_002_000" perms="rw" cached="false" setvar_vaddr="uio_map1" />
        <map mr="uio_map2" vaddr="0x30_004_000" perms="rw" cached="false" setvar_vaddr="uio_map2" /> -->

        <virtual_machine name="client_guest" id="0">
            <map mr="client_guest_ram" vaddr="0x40_000_000" perms="rwx" />
            <map mr="uio_map0" vaddr="0x30_000_000" perms="rw" cached="false" />
            <!-- <map mr="uio_map1" vaddr="0x30_002_000" perms="rw" cached="false" />
            <map mr="uio_map2" vaddr="0x30_004_000" perms="rw" cached="false" /> -->
            <map mr="gic_vcpu" vaddr="0x8_010_000" perms="rw" cached="false" /> 

            <map mr="uart" vaddr="0x9_000_000" perms="rw" cached="false" />
        </virtual_machine>
    </protection_domain>

    <channel>
        <end pd="driver_guest_vmm" id="3" />
        <end pd="client_guest_vmm" id="3" />
    </channel>
</system>